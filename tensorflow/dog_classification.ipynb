{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "dog-classification.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyPc1OrN16n3o+QgGT5gR/MR",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/JohannesBauer97/dog-classification/blob/main/dog_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5OyJcveaGxJ"
   },
   "source": [
    "# Dog Classification Network\n",
    "Train a model which can classify different dog breeds.\n",
    "\n",
    "Using the [Stanford Dogs Dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMJJVyp6dQsb"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JKh90OCcYsx-",
    "outputId": "0a5459d4-f5f3-4a7f-95dc-d44eab772b82",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import xml.etree.ElementTree as et\n",
    "import os\n",
    "from PIL import Image\n",
    "import sys\n",
    "import time\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "print(\"Tensorflow Version:\",tf.__version__)\n",
    "if IN_COLAB:\n",
    "    !nvidia-smi -L\n",
    "    "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEUynpOd2x2V"
   },
   "source": [
    "# Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1t75oNCk201M",
    "outputId": "e4abd384-80d1-4762-f264-5e0e34861474",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "image_dataset_url = \"http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\"\n",
    "images_dir = tf.keras.utils.get_file(\"Images\", image_dataset_url, untar=True)\n",
    "annotations_dataset_url = \"http://vision.stanford.edu/aditya86/ImageNetDogs/annotation.tar\"\n",
    "annotations_dir = tf.keras.utils.get_file(\"Annotation\", annotations_dataset_url, untar=True)\n",
    "target_dir = \"Cropped\"\n",
    "\n",
    "print(\"Images:\", images_dir, \"Annotation:\", annotations_dir)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciXzmK_vFLPv"
   },
   "source": [
    "# Crop Images\n",
    "Use the annotations to crop the images that only the dog is visible."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "a9yOinp1FJDN"
   },
   "source": [
    "os.mkdir(target_dir)\n",
    "breed_list = os.listdir(images_dir)\n",
    "counter = 1\n",
    "counter_max = len(breed_list)\n",
    "\n",
    "for breed in breed_list:\n",
    "    os.mkdir(target_dir + \"/\" + breed)\n",
    "    print(\"Created ({}/{}): {}/{}\".format(counter, counter_max, target_dir, breed))\n",
    "    counter += 1\n",
    "    for file in os.listdir(annotations_dir + \"/\" + breed):\n",
    "        img = Image.open(images_dir + \"/\" + breed + \"/\" + file + \".jpg\")\n",
    "        xml_tree = et.parse(annotations_dir + \"/\" + breed + \"/\" + file)\n",
    "        x_min = int(xml_tree.getroot().findall('object')[0].find('bndbox').find('xmin').text)\n",
    "        x_max = int(xml_tree.getroot().findall('object')[0].find('bndbox').find('xmax').text)\n",
    "        y_min = int(xml_tree.getroot().findall('object')[0].find('bndbox').find('ymin').text)\n",
    "        y_max = int(xml_tree.getroot().findall('object')[0].find('bndbox').find('ymax').text)\n",
    "        img = img.crop((x_min, y_min, x_max, y_max))\n",
    "        img = img.convert('RGB')\n",
    "        # img = img.resize((299, 299))\n",
    "        img.save(target_dir + \"/\" + breed + \"/\" + file + \".jpg\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# Prepare Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MqjeoJH7cXw1",
    "outputId": "18aa6eed-443e-4d8e-ee5b-970205798189",
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "img_height = 299\n",
    "img_width = 299\n",
    "batch_size = 16\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=target_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=6214,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "validation_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=target_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=9423,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "print(\"{} classes: {}\".format(num_classes, class_names))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "data_augmentation = tf.keras.Sequential(\n",
    "  [\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\", input_shape=(img_height, img_width, 3)),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip(\"vertical\", input_shape=(img_height, img_width, 3)),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.3),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomZoom(0.3),\n",
    "  ]\n",
    ")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# Model Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.Xception(\n",
    "        classes=num_classes,\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        # weights='imagenet',\n",
    ")\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    data_augmentation,\n",
    "    base_model\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"tmp/checkpoint\", save_weights_only=True, monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "epochs = 15\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=validation_ds,\n",
    "    epochs=epochs,\n",
    "    callbacks=[model_checkpoint_callback]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "version = int(time.time())\n",
    "model.save(\"model/{}\".format(version))\n",
    "\n",
    "#\n",
    "# Visualize Training\n",
    "#\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# Model Testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url = \"https://www.telegraph.co.uk/content/dam/science/2017/09/10/TELEMMGLPICT000107300056_trans_NvBQzQNjv4BqyuLFFzXshuGqnr8zPdDWXiTUh73-1IAIBaONvUINpkg.jpeg\"\n",
    "test_path = tf.keras.utils.get_file(\"test\", origin=test_url)\n",
    "\n",
    "img = tf.keras.preprocessing.image.load_img(test_path, target_size=(img_height, img_width))\n",
    "img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  }
 ]
}